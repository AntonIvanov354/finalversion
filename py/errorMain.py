# torch - –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
#
# transformers - –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –¥–ª—è –≥–æ—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –ò–ò
#
# datasets - –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
#
# rouge_score - –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
#
# nltk - –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞
#
# flask - –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞
import torch
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
    pipeline
)
from datasets import load_dataset
import numpy as np
from rouge_score import rouge_scorer
import nltk
from flask import Flask, request, jsonify
import pandas as pd
import warnings
import re

warnings.filterwarnings('ignore')

try:
    nltk.download('punkt', quiet=True)
except:
    pass


class RussianTextSummarizer:
    def __init__(self, model_name="IlyaGusev/rut5_base_sum_gazeta"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)
        self.model.to(self.device)
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")

    def split_text_into_chunks(self, text, max_chunk_size=2000):
        """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
        sentences = re.split(r'(?<=[.!?])\s+', text)

        chunks = []
        current_chunk = ""

        for sentence in sentences:
            # –ï—Å–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–µ –ø—Ä–µ–≤—ã—Å–∏—Ç –ª–∏–º–∏—Ç
            if len(current_chunk) + len(sentence) <= max_chunk_size:
                current_chunk += sentence + " "
            else:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π —á–∞–Ω–∫ –∏ –Ω–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—ã–π
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + " "

        # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞–Ω–∫
        if current_chunk:
            chunks.append(current_chunk.strip())

        return chunks

    def summarize_large_text(self, text, target_length=10000, max_chunk_size=2000):
        """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –±–æ–ª—å—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤"""
        if self.model is None:
            self.load_model()

        print(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏
        chunks = self.split_text_into_chunks(text, max_chunk_size)
        print(f"üìÅ –†–∞–∑–±–∏—Ç–æ –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")

        summaries = []
        total_summary_length = 0

        for i, chunk in enumerate(chunks, 1):
            print(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —á–∞—Å—Ç—å {i}/{len(chunks)}...")

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–ª–∏–Ω—É —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç—Ç–æ–π —á–∞—Å—Ç–∏
            chunk_ratio = len(chunk) / len(text)
            chunk_target_length = max(100, int(target_length * chunk_ratio))

            try:
                chunk_summary = self.summarize_text(
                    chunk,
                    max_length=chunk_target_length + 100,
                    min_length=chunk_target_length - 50
                )
                summaries.append(chunk_summary)
                total_summary_length += len(chunk_summary)

                print(f"‚úÖ –ß–∞—Å—Ç—å {i}: {len(chunk)} ‚Üí {len(chunk_summary)} —Å–∏–º–≤–æ–ª–æ–≤")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –≤ —á–∞—Å—Ç–∏ {i}: {e}")
                # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –¥–æ–±–∞–≤–ª—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫
                summaries.append(chunk[:500] + "...")

        # –ï—Å–ª–∏ —Å—É–º–º–∞—Ä–Ω–∞—è –¥–ª–∏–Ω–∞ –≤—Å–µ –µ—â–µ –±–æ–ª—å—à–µ —Ü–µ–ª–µ–≤–æ–π, –¥–µ–ª–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        if total_summary_length > target_length * 1.2:
            print("üéØ –î–µ–ª–∞—é —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é...")
            combined_summary = " ".join(summaries)
            final_summary = self.summarize_text(
                combined_summary,
                max_length=target_length + 1000,
                min_length=target_length - 1000
            )
            return final_summary
        else:
            return " ".join(summaries)

    def summarize_text(self, text, max_length=150, min_length=30, num_beams=4):
        """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (–±–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è)"""
        if self.model is None:
            self.load_model()

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        if len(text) > 3000:
            text = text[:3000] + "... [—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω]"

        inputs = self.tokenizer(
            text,
            max_length=1024,
            truncation=True,
            padding=True,
            return_tensors="pt"
        ).to(self.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                min_length=min_length,
                num_beams=num_beams,
                early_stopping=True,
                repetition_penalty=2.5,
                length_penalty=1.0,
                no_repeat_ngram_size=3
            )

        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return summary

    def preprocess_function(self, examples, max_input_length=512, max_target_length=128):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        inputs = examples["text"]
        targets = examples["summary"]

        model_inputs = self.tokenizer(
            inputs,
            max_length=max_input_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )

        labels = self.tokenizer(
            targets,
            max_length=max_target_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )

        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    def load_and_prepare_data(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

        try:
            dataset = load_dataset("IlyaGusev/gazeta", split="train[:500]")
            dataset = dataset.train_test_split(test_size=0.1, seed=42)

            tokenized_datasets = dataset.map(
                self.preprocess_function,
                batched=True,
                remove_columns=dataset["train"].column_names
            )

            return tokenized_datasets
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return self.create_sample_data()

    def create_sample_data(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ-–¥–∞–Ω–Ω—ã—Ö –µ—Å–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"""
        print("–°–æ–∑–¥–∞–Ω–∏–µ –¥–µ–º–æ-–¥–∞–Ω–Ω—ã—Ö...")

        sample_data = {
            "text": [
                "–†–æ—Å—Å–∏–π—Å–∫–∏–µ —É—á–µ–Ω—ã–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª–∏ –Ω–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤. –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–æ–∫–∞–∑–∞–ª–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤ 95% —Å–ª—É—á–∞–µ–≤ –∏ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å –ø—Ä–æ—Ü–µ—Å—Å –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏–π.",
                "–ö–æ–º–ø–∞–Ω–∏—è –Ø–Ω–¥–µ–∫—Å –∑–∞–ø—É—Å—Ç–∏–ª–∞ —Å–µ—Ä–≤–∏—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π. –°–∏—Å—Ç–µ–º–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º—ã –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∏ –ø–æ–º–æ–≥–∞–µ—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—è–º –±—ã—Å—Ç—Ä–µ–µ –Ω–∞—Ö–æ–¥–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–∞–±–æ—Ç—ã.",
                "–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª–∏ –∏–∑ –ú–ì–£ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∏ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞. –ú–æ–¥–µ–ª—å –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–æ–≥–∏ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã —Å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏."
            ],
            "summary": [
                "–£—á–µ–Ω—ã–µ —Å–æ–∑–¥–∞–ª–∏ –ò–ò –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö —Å–Ω–∏–º–∫–æ–≤ —Å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é 95%.",
                "–Ø–Ω–¥–µ–∫—Å –∑–∞–ø—É—Å—Ç–∏–ª —Å–µ—Ä–≤–∏—Å –∞–≤—Ç–æ—Ä–µ—Ñ–µ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.",
                "–ú–ì–£ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–ª —Ç–æ—á–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞."
            ]
        }

        from datasets import Dataset
        dataset = Dataset.from_dict(sample_data)
        dataset = dataset.train_test_split(test_size=0.3, seed=42)

        tokenized_datasets = dataset.map(
            self.preprocess_function,
            batched=True,
        )

        return tokenized_datasets

    def train_model(self, output_dir="./russian_summarizer"):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        print("üéØ –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")

        tokenized_datasets = self.load_and_prepare_data()

        training_args = Seq2SeqTrainingArguments(
            output_dir=output_dir,
            evaluation_strategy="epoch",
            learning_rate=2e-5,
            per_device_train_batch_size=2,
            per_device_eval_batch_size=2,
            weight_decay=0.01,
            save_total_limit=2,
            num_train_epochs=2,
            predict_with_generate=True,
            fp16=torch.cuda.is_available(),
            logging_steps=10,
            save_steps=100,
            warmup_steps=50,
        )

        data_collator = DataCollatorForSeq2Seq(
            self.tokenizer,
            model=self.model,
            padding=True
        )

        trainer = Seq2SeqTrainer(
            model=self.model,
            args=training_args,
            train_dataset=tokenized_datasets["train"],
            eval_dataset=tokenized_datasets["test"],
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )

        print("üîÑ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
        trainer.train()

        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)

        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {output_dir}")
        return trainer

    def evaluate_summary(self, original_text, generated_summary, reference_summary=None):
        """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏"""
        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

        if reference_summary:
            scores = scorer.score(reference_summary, generated_summary)
            print("=== –û–¶–ï–ù–ö–ê ROUGE ===")
            for key in scores:
                print(
                    f"{key}: Precision: {scores[key].precision:.3f}, Recall: {scores[key].recall:.3f}, F1: {scores[key].fmeasure:.3f}")

        original_words = len(original_text.split())
        summary_words = len(generated_summary.split())
        compression_ratio = original_words / summary_words if summary_words > 0 else 0

        print(f"\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(original_text)} —Å–∏–º–≤–æ–ª–æ–≤, {original_words} —Å–ª–æ–≤")
        print(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {len(generated_summary)} —Å–∏–º–≤–æ–ª–æ–≤, {summary_words} —Å–ª–æ–≤")
        print(f"–ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å–∂–∞—Ç–∏—è: {compression_ratio:.1f}x")

        return {
            'original_length': original_words,
            'summary_length': summary_words,
            'compression_ratio': compression_ratio
        }


def process_large_text_file(input_file, output_file, target_chars=10000):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
    summarizer = RussianTextSummarizer()
    summarizer.load_model()

    try:
        # –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞
        with open(input_file, 'r', encoding='utf-8') as f:
            text = f.read()

        print(f"üìñ –ü—Ä–æ—á–∏—Ç–∞–Ω —Ñ–∞–π–ª: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

        if len(text) > 150000:
            print("‚ö†Ô∏è –¢–µ–∫—Å—Ç –ø—Ä–µ–≤—ã—à–∞–µ—Ç 150,000 —Å–∏–º–≤–æ–ª–æ–≤, –æ–±—Ä–µ–∑–∞—é...")
            text = text[:150000]

        # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        print("üéØ –ù–∞—á–∏–Ω–∞—é —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é...")
        summary = summarizer.summarize_large_text(text, target_length=target_chars)

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("=== –†–ï–ó–£–õ–¨–¢–ê–¢ –°–£–ú–ú–ê–†–ò–ó–ê–¶–ò–ò ===\n\n")
            f.write(summary)
            f.write(f"\n\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===\n")
            f.write(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è: {len(summary)} —Å–∏–º–≤–æ–ª–æ–≤\n")
            f.write(f"–°–∂–∞—Ç–∏–µ: {len(text) / len(summary):.1f}x\n")

        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        print(f"üìä –ò—Ç–æ–≥: {len(text)} ‚Üí {len(summary)} —Å–∏–º–≤–æ–ª–æ–≤")

        return summary

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        return None


def interactive_large_text_summarizer():#fileNameBase, fileName):
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ —Ç–µ–∫—Å—Ç–∞–º–∏"""
#    print("üá∑üá∫ –°–£–ú–ú–ê–†–ò–ó–ê–¢–û–† –ë–û–õ–¨–®–ò–• –¢–ï–ö–°–¢–û–í")
 #   print("=" * 50)

    summarizer = RussianTextSummarizer()
    summarizer.load_model()

    while True:
  #      print("\n" + "=" * 50)
   #     print("1 üìù –í–≤–µ—Å—Ç–∏ —Ç–µ–∫—Å—Ç –≤—Ä—É—á–Ω—É—é")
    #    print("2 üìÅ –û–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª")
     #   print("3 üö™ –í—ã—Ö–æ–¥")
      #  print("=" * 50)
        filename = "/home/abama/Pictures/finalversion/py/txt/file.txt"

        if not filename:
            filename = filename + ".txt"

        output_file = filename.replace('.txt', '_summary.txt')

        if process_large_text_file(filename, output_file):
            print(f"üéâ –§–∞–π–ª –æ–±—Ä–∞–±–æ—Ç–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞")
""" if choice == "1":
            print("üìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç (–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –¥–æ 150,000 —Å–∏–º–≤–æ–ª–æ–≤):")
            print("(–≤–≤–µ–¥–∏—Ç–µ '–ö–û–ù–ï–¶' –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–µ –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")

            lines = []
            total_chars = 0

            while True:
                line = input()
                if line.strip() == '–ö–û–ù–ï–¶':
                    break
                lines.append(line)
                total_chars += len(line)

                if total_chars >= 150000:
                    print("‚ö†Ô∏è –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤ 150,000 —Å–∏–º–≤–æ–ª–æ–≤")
                    break

            text = "".join(lines)

            if text:
                print(f"üìä –¢–µ–∫—Å—Ç –ø–æ–ª—É—á–µ–Ω: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
                target_length = min(10000, len(text) // 15)  # –¶–µ–ª–µ–≤–∞—è –¥–ª–∏–Ω–∞ ~6.7% –æ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª–∞

                summary = summarizer.summarize_large_text(text, target_length)
                print("‚úÖ –†–ï–ó–£–õ–¨–¢–ê–¢:")
                print("‚Äî" * 50)
                print(summary)
                print("‚Äî" * 50)
                print(f"üìä –°–∂–∞—Ç–∏–µ: {len(text)} ‚Üí {len(summary)} —Å–∏–º–≤–æ–ª–æ–≤")

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                save = input("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç? (y/n): ").lower()
                if save == 'y':
                    filename = input("–ò–º—è —Ñ–∞–π–ª–∞ (–±–µ–∑ .txt): ").strip()
                    if not filename:
                        filename = "summary_result"

                    with open(f"{filename}.txt", 'w', encoding='utf-8') as f:
                        f.write(summary)
                    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {filename}.txt")
            else:
                print("‚ùå –¢–µ–∫—Å—Ç –Ω–µ –≤–≤–µ–¥–µ–Ω!"

        elif choice == "2":

        elif choice == "3":
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        else:
            print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä!")"""


def main():#fileNameBase, fileName):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    print("üá∑üá∫ RUSSIAN LARGE TEXT SUMMARIZATION AI")
    print("=" * 60)
    print("üìä –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –¥–æ 150,000 —Å–∏–º–≤–æ–ª–æ–≤")
    print("üéØ –°–æ–∫—Ä–∞—â–∞–µ—Ç –¥–æ ~10,000 —Å–∏–º–≤–æ–ª–æ–≤")
    print("=" * 60)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
    if torch.cuda.is_available():
        print(f"üéØ GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {torch.cuda.get_device_name(0)}")
    else:
        print("‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

    # –ó–∞–ø—É—Å–∫ –∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
    interactive_large_text_summarizer()#fileNameBase, fileName)
main()